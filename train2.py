import os
import argparse
import json
import time
from datetime import datetime

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import autocast, GradScaler

from sub_model.conflict_model import UAVConflictModel
from trainer.losses import OccupancyLoss, MotionLoss, MultiTaskLoss
from trainer.metrics import Metrics
from trainer.risk_evaluator import RiskEvaluator
from image_dataset import UAVImageDataset, UAVSimpleImageDataset
from validation_visualizer import ValidationVisualizer


class Trainer:
    def __init__(self, config):
        self.config = config

        # âœ… è®¾å¤‡åˆå§‹åŒ– - ç»Ÿä¸€ç®¡ç†
        if torch.cuda.is_available() and config.device == 'cuda':
            self.device = torch.device('cuda')
            torch.cuda.empty_cache()
            print(f"[INFO] Using GPU: {torch.cuda.get_device_name(0)}")
        else:
            self.device = torch.device('cpu')
            print("[WARNING] CUDA not available, using CPU")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = os.path.join(
            config.output_dir, f"{config.mode}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        os.makedirs(os.path.join(self.output_dir, 'checkpoints'), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'visualizations'), exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        print("=" * 60)
        print("æ­¥éª¤ 1/3: æ­£åœ¨åˆå§‹åŒ–æ•°æ®é›†...")
        print("=" * 60)
        self._init_data()

        print("\n" + "=" * 60)
        print("æ­¥éª¤ 2/3: æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        print("=" * 60)
        self._init_model()

        print("\n" + "=" * 60)
        print("æ­¥éª¤ 3/3: æ­£åœ¨åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        print("=" * 60)
        self._init_optimizer()

        self.use_amp = config.use_amp and torch.cuda.is_available()
        self.scaler = GradScaler() if self.use_amp else None

        # âœ… ç¡®ä¿ metrics å’Œ risk_evaluator çŸ¥é“è®¾å¤‡
        self.metrics = Metrics(config.risk_w_var, config.risk_w_ent, config.risk_w_temp)
        self.risk_evaluator = RiskEvaluator(config.risk_w_var, config.risk_w_ent, config.risk_w_temp)

        self.visualizer = ValidationVisualizer(
            os.path.join(self.output_dir, 'visualizations'),
            mode=config.mode
        )

        self.best_val_loss = float('inf')

        # ä¿å­˜é…ç½®
        with open(os.path.join(self.output_dir, 'config.json'), 'w') as f:
            json.dump(vars(config), f, indent=2)

        print(f"\n{'=' * 60}")
        print("åˆå§‹åŒ–å®Œæˆï¼é…ç½®ä¿¡æ¯ï¼š")
        print(f"{'=' * 60}")
        print(f"Training mode: {config.mode}")
        print(f"Device: {self.device}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            print(f"AMP: {'Enabled' if self.use_amp else 'Disabled'}")
        print(f"Batch size: {config.batch_size}")
        print(f"Accumulation steps: {config.accum_steps}")
        print(f"Effective batch size: {config.batch_size * config.accum_steps}")
        print(f"{'=' * 60}\n")

    def _init_data(self):
        """åˆå§‹åŒ–æ•°æ®é›†"""
        use_occ = self.config.mode in ['occupancy', 'multitask']
        use_motion = self.config.mode in ['motion', 'multitask']
        img_size = tuple(self.config.img_size) if self.config.img_size else (640, 640)

        print(f"é…ç½®ä¿¡æ¯:")
        print(f"  - æ•°æ®ç›®å½•: {self.config.data_dir}")
        print(f"  - æ•°æ®é›†ç±»å‹: {self.config.dataset_type}")
        print(f"  - å›¾åƒå°ºå¯¸: {img_size}")
        print(f"  - å†å²å¸§æ•°: {self.config.history_frames}")
        print(f"  - ä½¿ç”¨occupancy: {use_occ}")
        print(f"  - ä½¿ç”¨motion: {use_motion}")

        try:
            print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
            if self.config.dataset_type == 'sequence':
                full_dataset = UAVImageDataset(
                    root_dir=self.config.data_dir,
                    history_frames=self.config.history_frames,
                    use_occ=use_occ,
                    use_motion=use_motion,
                    img_size=img_size
                )
            else:
                full_dataset = UAVSimpleImageDataset(
                    root_dir=self.config.data_dir,
                    img_size=img_size,
                    use_occ=use_occ,
                    use_motion=use_motion
                )

            print(f"âœ“ æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œæ€»æ ·æœ¬æ•°: {len(full_dataset)}")

            if len(full_dataset) == 0:
                raise ValueError("æ•°æ®é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®ç›®å½•å’Œæ–‡ä»¶æ ¼å¼ã€‚")

        except Exception as e:
            print(f"âœ— æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        val_size = int(len(full_dataset) * self.config.val_ratio)
        train_size = len(full_dataset) - val_size

        print(f"\næ­£åœ¨åˆ’åˆ†æ•°æ®é›†...")
        generator = torch.Generator().manual_seed(self.config.seed)
        self.train_dataset, self.val_dataset = random_split(
            full_dataset, [train_size, val_size], generator=generator
        )

        # DataLoaderè®¾ç½®
        num_workers = 0 if os.name == 'nt' else min(self.config.num_workers, 2)

        print(f"\næ­£åœ¨åˆ›å»ºDataLoader...")
        # âœ… ç¡®ä¿ pin_memory æ­£ç¡®è®¾ç½®
        pin_memory = torch.cuda.is_available()

        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=True,
            persistent_workers=False
        )
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=False
        )

        print(f"âœ“ DataLoaderåˆ›å»ºæˆåŠŸ")
        print(f"  - è®­ç»ƒé›†: {train_size} æ ·æœ¬, {len(self.train_loader)} batches")
        print(f"  - éªŒè¯é›†: {val_size} æ ·æœ¬, {len(self.val_loader)} batches")
        print(f"  - Num workers: {num_workers}")
        print(f"  - Pin memory: {pin_memory}")

    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹å’ŒæŸå¤±å‡½æ•°"""
        try:
            print("æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
            self.model = UAVConflictModel(
                mode=self.config.mode,
                hidden_dim=self.config.hidden_dim,
                modes=self.config.num_modes,
                encoder_backbone=self.config.backbone,
                future_steps=self.config.future_steps
            ).to(self.device)  # âœ… ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š

            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            print(f"  - Backbone: {self.config.backbone}")
            print(f"  - Hidden dim: {self.config.hidden_dim}")
            print(f"  - æ€»å‚æ•°é‡: {total_params:,}")
            print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        except Exception as e:
            print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        try:
            print("\næ­£åœ¨åˆ›å»ºæŸå¤±å‡½æ•°...")
            if self.config.mode == 'multitask':
                self.criterion = MultiTaskLoss(
                    risk_weight=self.config.risk_weight,
                    risk_w_var=self.config.risk_w_var,
                    risk_w_ent=self.config.risk_w_ent,
                    risk_w_temp=self.config.risk_w_temp,
                    occ_weight=self.config.occ_weight,
                    motion_weight=self.config.motion_weight
                ).to(self.device)  # âœ… ç¡®ä¿ criterion åœ¨ GPU ä¸Š
            elif self.config.mode == 'occupancy':
                self.criterion = OccupancyLoss(
                    self.config.risk_weight,
                    self.config.risk_w_var,
                    self.config.risk_w_ent,
                    self.config.risk_w_temp
                ).to(self.device)  # âœ… ç¡®ä¿ criterion åœ¨ GPU ä¸Š
            else:
                self.criterion = MotionLoss().to(self.device)  # âœ… ç¡®ä¿ criterion åœ¨ GPU ä¸Š

            print(f"âœ“ æŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ ({self.config.mode} mode)")

        except Exception as e:
            print(f"âœ— æŸå¤±å‡½æ•°åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        params = list(self.model.parameters())
        if hasattr(self.criterion, 'parameters'):
            params += list(self.criterion.parameters())

        print("æ­£åœ¨åˆ›å»ºä¼˜åŒ–å™¨...")
        self.optimizer = AdamW(
            params,
            lr=self.config.lr,
            weight_decay=self.config.weight_decay,
            eps=1e-8
        )

        if self.config.scheduler == 'cosine':
            self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.config.epochs)
            print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ (AdamW + CosineAnnealing)")
        else:
            self.scheduler = None
            print(f"âœ“ ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ (AdamW)")

        print(f"  - Learning rate: {self.config.lr}")
        print(f"  - Weight decay: {self.config.weight_decay}")

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_risk = 0
        batch_count = 0

        total_batches = len(self.train_loader)
        self.optimizer.zero_grad()

        epoch_start = time.time()

        print(f"è®­ç»ƒè¿›åº¦: ", end='', flush=True)

        for batch_idx, batch_data in enumerate(self.train_loader):
            try:
                # âœ… è§£åŒ…æ•°æ®å¹¶ç§»åˆ° GPU
                history, targets = batch_data

                # âœ… ç§»åŠ¨åˆ°è®¾å¤‡ (non_blocking åŠ é€Ÿ)
                history = history.to(self.device, non_blocking=True)

                # âœ… ç¡®ä¿æ‰€æœ‰ targets éƒ½åœ¨ GPU ä¸Š
                targets_gpu = {}
                for k, v in targets.items():
                    if isinstance(v, torch.Tensor):
                        targets_gpu[k] = v.to(self.device, non_blocking=True)
                    else:
                        targets_gpu[k] = v

                # å‰å‘ä¼ æ’­
                with torch.amp.autocast("cuda", enabled=self.use_amp):
                    outputs = self.model(history, return_confidence=True)

                    if self.config.mode == 'multitask':
                        losses = self.criterion(outputs, targets_gpu)
                        loss = losses['total']

                        occ_pred, _ = outputs['occ']
                        risks = self.risk_evaluator.compute_all_risks(
                            torch.sigmoid(occ_pred),
                            differentiable=True
                        )
                        total_risk += risks['combined'].mean().item()

                    elif self.config.mode == 'occupancy':
                        pred, conf = outputs
                        loss, details = self.criterion(pred, targets_gpu['occ'], conf)

                        risks = self.risk_evaluator.compute_all_risks(
                            torch.sigmoid(pred),
                            differentiable=True
                        )
                        total_risk += risks['combined'].mean().item()

                    else:  # motion
                        pred, conf = outputs
                        loss, details = self.criterion(pred, targets_gpu['motion'], conf)

                # åå‘ä¼ æ’­
                loss = loss / self.config.accum_steps

                if self.use_amp:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()

                # ä¼˜åŒ–å™¨æ­¥è¿›
                if (batch_idx + 1) % self.config.accum_steps == 0:
                    if self.use_amp:
                        self.scaler.unscale_(self.optimizer)

                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)

                    if self.use_amp:
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        self.optimizer.step()

                    self.optimizer.zero_grad()

                total_loss += loss.item() * self.config.accum_steps
                batch_count += 1

                # æ›´æ–°è¿›åº¦æ¡
                progress = (batch_idx + 1) / total_batches
                bar_length = 40
                filled = int(bar_length * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

                avg_loss = total_loss / batch_count
                elapsed = time.time() - epoch_start
                eta = elapsed / progress - elapsed if progress > 0 else 0

                print(f"\rè®­ç»ƒè¿›åº¦: [{bar}] {progress * 100:.1f}% | Loss: {avg_loss:.4f} | ETA: {eta / 60:.1f}min",
                      end='', flush=True)

            except Exception as e:
                print(f"\nâœ— [ERROR] Batch {batch_idx} failed: {e}")
                import traceback
                traceback.print_exc()
                continue

        print()  # æ¢è¡Œ
        total_time = time.time() - epoch_start
        avg_loss = total_loss / max(batch_count, 1)
        avg_risk = total_risk / max(batch_count, 1)

        print(f"âœ“ è®­ç»ƒå®Œæˆ | è€—æ—¶: {total_time / 60:.1f}min | å¹³å‡Loss: {avg_loss:.4f} | Risk: {avg_risk:.6f}")

        return avg_loss, avg_risk

    def validate(self, epoch):
        """éªŒè¯ - GPU ä¼˜åŒ–ç‰ˆæœ¬"""
        self.model.eval()
        total_loss = 0
        total_risk = 0
        all_metrics = {}

        vis_count = 0
        total_batches = len(self.val_loader)

        val_start = time.time()

        print(f"éªŒè¯è¿›åº¦: ", end='', flush=True)

        with torch.no_grad():
            for batch_idx, (history, targets) in enumerate(self.val_loader):
                try:
                    # âœ… ç§»åŠ¨åˆ° GPU
                    history = history.to(self.device, non_blocking=True)

                    # âœ… ç¡®ä¿æ‰€æœ‰ targets éƒ½åœ¨ GPU ä¸Š
                    targets_gpu = {}
                    for k, v in targets.items():
                        if isinstance(v, torch.Tensor):
                            targets_gpu[k] = v.to(self.device, non_blocking=True)
                        else:
                            targets_gpu[k] = v

                    # å‰å‘ä¼ æ’­
                    outputs = self.model(history, return_confidence=True)

                    if self.config.mode == 'multitask':
                        losses = self.criterion(outputs, targets_gpu)
                        loss = losses['total']

                        occ_pred, _ = outputs['occ']
                        risks = self.risk_evaluator.compute_all_risks(
                            torch.sigmoid(occ_pred),
                            differentiable=False
                        )
                        total_risk += risks['combined'].mean().item()

                        if 'occ' in targets_gpu:
                            metrics = self.metrics.compute_occ_metrics(occ_pred, targets_gpu['occ'])
                            for k, v in metrics.items():
                                all_metrics[k] = all_metrics.get(k, 0) + v

                    elif self.config.mode == 'occupancy':
                        pred, conf = outputs
                        loss, _ = self.criterion(pred, targets_gpu['occ'], conf)

                        risks = self.risk_evaluator.compute_all_risks(
                            torch.sigmoid(pred),
                            differentiable=False
                        )
                        total_risk += risks['combined'].mean().item()

                        metrics = self.metrics.compute_occ_metrics(pred, targets_gpu['occ'])
                        for k, v in metrics.items():
                            all_metrics[k] = all_metrics.get(k, 0) + v

                    else:  # motion
                        pred, conf = outputs
                        loss, _ = self.criterion(pred, targets_gpu['motion'], conf)

                        metrics = self.metrics.compute_motion_metrics(pred, targets_gpu['motion'])
                        for k, v in metrics.items():
                            all_metrics[k] = all_metrics.get(k, 0) + v

                    total_loss += loss.item()

                    # âœ… å¯è§†åŒ–æ—¶å°†æ•°æ®ç§»å› CPU
                    if vis_count < self.config.max_vis_batches:
                        history_cpu = history.cpu()
                        targets_cpu = {k: v.cpu() if isinstance(v, torch.Tensor) else v
                                       for k, v in targets_gpu.items()}

                        if self.config.mode == 'multitask':
                            outputs_cpu = {
                                'occ': (outputs['occ'][0].cpu(), outputs['occ'][1].cpu()),
                                'motion': (outputs['motion'][0].cpu(), outputs['motion'][1].cpu())
                            }
                        elif self.config.mode == 'occupancy':
                            outputs_cpu = (outputs[0].cpu(), outputs[1].cpu())
                        else:
                            outputs_cpu = (outputs[0].cpu(), outputs[1].cpu())

                        self.visualizer.visualize_batch(
                            history_cpu, outputs_cpu, targets_cpu,
                            epoch=epoch, batch_idx=batch_idx,
                            max_samples=self.config.max_vis_samples
                        )
                        vis_count += 1

                    # è¿›åº¦æ¡æ›´æ–°
                    progress = (batch_idx + 1) / total_batches
                    bar_length = 40
                    filled = int(bar_length * progress)
                    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

                    print(f"\réªŒè¯è¿›åº¦: [{bar}] {progress * 100:.1f}%", end='', flush=True)

                except Exception as e:
                    print(f"\nâš  [WARNING] Validation batch {batch_idx} failed: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

        print()  # æ¢è¡Œ
        val_time = time.time() - val_start
        avg_loss = total_loss / len(self.val_loader)
        avg_metrics = {k: v / len(self.val_loader) for k, v in all_metrics.items()}
        avg_risk = total_risk / len(self.val_loader)

        print(f"âœ“ éªŒè¯å®Œæˆ | è€—æ—¶: {val_time:.1f}s | Loss: {avg_loss:.4f} | Risk: {avg_risk:.6f}")

        return avg_loss, avg_metrics, avg_risk

    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        ckpt = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
        }
        path = os.path.join(self.output_dir, 'checkpoints', f'epoch_{epoch}.pt')
        torch.save(ckpt, path)
        if is_best:
            best_path = os.path.join(self.output_dir, 'checkpoints', 'best.pt')
            torch.save(ckpt, best_path)
            print(f"  âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹è®­ç»ƒ")
        print(f"{'=' * 60}")
        print(f"æ€»Epochs: {self.config.epochs}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"{'=' * 60}\n")

        for epoch in range(1, self.config.epochs + 1):
            print(f"\n{'â”' * 60}")
            print(f"EPOCH {epoch}/{self.config.epochs}")
            print(f"{'â”' * 60}")

            # è®­ç»ƒ
            train_loss, train_risk = self.train_epoch(epoch)

            # éªŒè¯
            val_loss, val_metrics, val_risk = self.validate(epoch)

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            lr = self.optimizer.param_groups[0]['lr']

            # ç»“æœæ±‡æ€»
            print(f"{'â”€' * 60}")
            print(f"Epoch {epoch} æ±‡æ€» | LR: {lr:.6f}")
            print(f"  è®­ç»ƒ: Loss={train_loss:.4f}, Risk={train_risk:.6f}")
            print(f"  éªŒè¯: Loss={val_loss:.4f}, Risk={val_risk:.6f}, IoU={val_metrics.get('iou', 0):.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_loss < self.best_val_loss
            if is_best:
                self.best_val_loss = val_loss
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼")

            # å®šæœŸä¿å­˜
            if epoch % self.config.save_interval == 0 or is_best:
                self.save_checkpoint(epoch, is_best)

        print(f"\n{'â”' * 60}")
        print(f"è®­ç»ƒå®Œæˆï¼")
        print(f"{'â”—' * 60}")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"æ¨¡å‹ä¿å­˜ä½ç½®: {self.output_dir}")
        print(f"{'â”€' * 60}\n")


def parse_args():
    p = argparse.ArgumentParser(description='UAV Conflict Model Trainer')

    # æ•°æ®ç›¸å…³
    p.add_argument('--data_dir', default=r'D:\model_12.22_fixed\images',
                   help='æ•°æ®é›†æ ¹ç›®å½•')
    p.add_argument('--dataset_type', default='sequence', choices=['sequence', 'simple'],
                   help='æ•°æ®é›†ç±»å‹')
    p.add_argument('--history_frames', type=int, default=9,
                   help='å†å²å¸§æ•°')
    p.add_argument('--output_dir', default='outputs',
                   help='è¾“å‡ºç›®å½•')
    p.add_argument('--val_ratio', type=float, default=0.2,
                   help='éªŒè¯é›†æ¯”ä¾‹')
    p.add_argument('--img_size', type=int, nargs=2, default=[640, 640],
                   help='å›¾åƒå°ºå¯¸')

    # æ¨¡å‹ç›¸å…³
    p.add_argument('--mode', default='occupancy', choices=['multitask', 'occupancy', 'motion'],
                   help='è®­ç»ƒæ¨¡å¼')
    p.add_argument('--backbone', default='resnet50',
                   help='ç¼–ç å™¨backbone')
    p.add_argument('--hidden_dim', type=int, default=128,
                   help='éšè—å±‚ç»´åº¦')
    p.add_argument('--num_modes', type=int, default=5,
                   help='æ¨¡æ€æ•°é‡')
    p.add_argument('--future_steps', type=int, default=1,
                   help='é¢„æµ‹æœªæ¥æ­¥æ•°')

    # è®­ç»ƒç›¸å…³
    p.add_argument('--epochs', type=int, default=100,
                   help='è®­ç»ƒè½®æ•°')
    p.add_argument('--batch_size', type=int, default=2,
                   help='æ‰¹æ¬¡å¤§å°')
    p.add_argument('--accum_steps', type=int, default=4,
                   help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    p.add_argument('--lr', type=float, default=1e-4,
                   help='å­¦ä¹ ç‡')
    p.add_argument('--weight_decay', type=float, default=1e-4,
                   help='æƒé‡è¡°å‡')
    p.add_argument('--grad_clip', type=float, default=1.0,
                   help='æ¢¯åº¦è£å‰ª')
    p.add_argument('--scheduler', default='cosine', choices=['cosine', 'none'],
                   help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    p.add_argument('--use_amp', action='store_true', default=False,
                   help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')

    # Riskç›¸å…³
    p.add_argument('--risk_weight', type=float, default=0.01)
    p.add_argument('--risk_w_var', type=float, default=1.0)
    p.add_argument('--risk_w_ent', type=float, default=0.5)
    p.add_argument('--risk_w_temp', type=float, default=0.3)

    # MultiTaskæƒé‡
    p.add_argument('--occ_weight', type=float, default=1.0)
    p.add_argument('--motion_weight', type=float, default=1.0)

    # å¯è§†åŒ–ç›¸å…³
    p.add_argument('--max_vis_batches', type=int, default=3)
    p.add_argument('--max_vis_samples', type=int, default=3)

    # å…¶ä»–
    p.add_argument('--num_workers', type=int, default=1,
                   help='DataLoaderå·¥ä½œè¿›ç¨‹æ•°')
    p.add_argument('--device', default='cuda',
                   help='è®­ç»ƒè®¾å¤‡')
    p.add_argument('--seed', type=int, default=42,
                   help='éšæœºç§å­')
    p.add_argument('--save_interval', type=int, default=10,
                   help='ä¿å­˜é—´éš”')

    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    print("=" * 60)
    print("UAV Conflict Model Trainer - GPU Optimized")
    print("=" * 60)
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print("=" * 60)

    try:
        trainer = Trainer(args)
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼")
    except Exception as e:
        print(f"\n{'=' * 60}")
        print("âœ— è®­ç»ƒå¤±è´¥ï¼")
        print(f"{'=' * 60}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print(f"\nè¯¦ç»†å †æ ˆ:")
        import traceback

        traceback.print_exc()
        print(f"\n{'=' * 60}")
        print("è°ƒè¯•å»ºè®®:")
        print("1. æ£€æŸ¥ä¸Šé¢çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        print("2. ç¡®è®¤æ•°æ®è·¯å¾„å’Œæ ¼å¼æ­£ç¡®")
        print("3. æ£€æŸ¥GPUæ˜¾å­˜æ˜¯å¦å……è¶³")
        print("4. å°è¯•å‡å°batch_size")
        print(f"{'=' * 60}\n")